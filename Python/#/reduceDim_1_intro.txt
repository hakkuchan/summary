2. 低方差滤波（Low Variance Filter）
如果我们有一个数据集，其中某列的数值基本一致，也就是它的方差非常低，那么这个变量还有价值吗？
和上一种方法的思路一致，我们通常认为低方差变量携带的信息量也很少，所以可以把它直接删除。
放到实践中，就是先计算所有变量的方差大小，然后删去其中最小的几个。
需要注意的一点是：方差与数据范围相关的，因此在采用该方法前需要对数据做归一化处理。

3. 高相关滤波（High Correlation filter）
如果两个变量之间是高度相关的，这意味着它们具有相似的趋势并且可能携带类似的信息。
同理，这类变量的存在会降低某些模型的性能（例如线性和逻辑回归模型）。
为了解决这个问题，我们可以计算独立数值变量之间的相关性。如果相关系数超过某个阈值，就删除其中一个变量。
作为一般准则，我们应该保留那些与目标变量显示相当或高相关性的变量。

4. 随机森林（Random Forest）
随机森林是一种广泛使用的特征选择算法，它会自动计算各个特征的重要性。

5. 反向特征消除（Backward Feature Elimination）
以下是反向特征消除的主要步骤：
· 先获取数据集中的全部n个变量，然后用它们训练一个模型
· 计算模型的性能
· 在删除每个变量（n次）后计算模型的性能，即我们每次都去掉一个变量，用剩余的n-1个变量训练模型。
· 确定对模型性能影响最小的变量，把它删除
· 重复此过程，直到不再能删除任何变量

6. 前向特征选择（Forward Feature Selection）
前向特征选择其实就是反向特征消除的相反过程，即找到能改善模型性能的最佳特征，而不是删除弱影响特征。它背后的思路如下所述：
· 选择一个特征，用每个特征训练模型n次，得到n个模型。
· 选择模型性能最佳的变量作为初始变量。
· 每次添加一个变量继续训练，重复上一过程，最后保留性能提升最大的变量。
· 一直添加，一直筛选，直到模型性能不再有明显提高。
