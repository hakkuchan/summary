"""
1. 缺失值比率（Missing Value Ratio）
假设你有一个数据集，你第一步会做什么？
在构建模型前，对数据进行探索性分析必不可少。
但在浏览数据的过程中，有时候我们会发现其中包含不少缺失值。
如果缺失值少，我们可以填补缺失值或直接删除这个变量；如果缺失值过多，你会怎么办呢？
当缺失值在数据集中的占比过高时，一般我会选择直接删除这个变量，因为它包含的信息太少了。
但具体删不删、怎么删需要视情况而定，我们可以设置一个阈值，如果缺失值占比高于阈值，删除它所在的列。
阈值越高，降维方法越积极。


2. 低方差滤波（Low Variance Filter）
如果我们有一个数据集，其中某列的数值基本一致，也就是它的方差非常低，那么这个变量还有价值吗？
和上一种方法的思路一致，我们通常认为低方差变量携带的信息量也很少，所以可以把它直接删除。
放到实践中，就是先计算所有变量的方差大小，然后删去其中最小的几个。
需要注意的一点是：方差与数据范围相关的，因此在采用该方法前需要对数据做归一化处理。

3. 高相关滤波（High Correlation filter）
如果两个变量之间是高度相关的，这意味着它们具有相似的趋势并且可能携带类似的信息。
同理，这类变量的存在会降低某些模型的性能（例如线性和逻辑回归模型）。
为了解决这个问题，我们可以计算独立数值变量之间的相关性。如果相关系数超过某个阈值，就删除其中一个变量。
作为一般准则，我们应该保留那些与目标变量显示相当或高相关性的变量。

4. 随机森林（Random Forest）
随机森林是一种广泛使用的特征选择算法，它会自动计算各个特征的重要性。

5. 反向特征消除（Backward Feature Elimination）
以下是反向特征消除的主要步骤：
· 先获取数据集中的全部n个变量，然后用它们训练一个模型
· 计算模型的性能
· 在删除每个变量（n次）后计算模型的性能，即我们每次都去掉一个变量，用剩余的n-1个变量训练模型。
· 确定对模型性能影响最小的变量，把它删除
· 重复此过程，直到不再能删除任何变量

6. 前向特征选择（Forward Feature Selection）
前向特征选择其实就是反向特征消除的相反过程，即找到能改善模型性能的最佳特征，而不是删除弱影响特征。它背后的思路如下所述：
· 选择一个特征，用每个特征训练模型n次，得到n个模型。
· 选择模型性能最佳的变量作为初始变量。
· 每次添加一个变量继续训练，重复上一过程，最后保留性能提升最大的变量。
· 一直添加，一直筛选，直到模型性能不再有明显提高。

7. 因子分析（Factor Analysis）
因子分析是一种常见的统计方法，它能从多个变量中提取共性因子，并得到最优解。
假设我们有两个变量：收入和教育。
它们可能是高度相关的，因为总体来看，学历高的人一般收入也更高，反之亦然。所以它们可能存在一个潜在的共性因子，比如“能力”。
在因子分析中，我们将变量按其相关性分组，即特定组内所有变量的相关性较高，组间变量的相关性较低。
我们把每个组称为一个因子，它是多个变量的组合。和原始数据集的变量相比，这些因子在数量上更少，但携带的信息基本一致。

8. 主成分分析（PCA）

如果说因子分析是假设存在一系列潜在因子，能反映变量携带的信息，
那PCA就是通过正交变换将原始的n维数据集变换到一个新的被称做主成分的数据集中，
即从现有的大量变量中提取一组新的变量。下面是关于PCA的一些要点：
· 主成分是原始变量的线性组合。
· 第一个主成分具有最大的方差值。
· 第二主成分试图解释数据集中的剩余方差，并且与第一主成分不相关（正交）。
· 第三主成分试图解释前两个主成分等没有解释的方差。

9. 独立分量分析（ICA）
独立分量分析（ICA）基于信息理论，是最广泛使用的降维技术之一。
PCA和ICA之间的主要区别在于，PCA寻找不相关的因素，而ICA寻找独立因素。
如果两个变量不相关，它们之间就没有线性关系。如果它们是独立的，它们就不依赖于其他变量。
例如，一个人的年龄和他吃了什么/看了什么电视无关。
该算法假设给定变量是一些未知潜在变量的线性混合。
它还假设这些潜在变量是相互独立的，即它们不依赖于其他变量，因此它们被称为观察数据的独立分量。

10. 流形学习
Isomap
TSNE
UMap
……
"""